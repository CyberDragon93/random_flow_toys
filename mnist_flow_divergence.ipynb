{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505148f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rectified_flow.rectified_flow import RectifiedFlow\n",
    "from rectified_flow.utils import match_dim_with_data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "transform_list = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms.Compose(transform_list)\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,          \n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)  # torch.Size([256, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1177dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"unet\"\n",
    "from rectified_flow.models.enhanced_mlp import VarMLP\n",
    "from rectified_flow.models.utils import EMAModel\n",
    "from rectified_flow.models.unet import SongUNet, SongUNetConfig\n",
    "\n",
    "flow_model = SongUNet.from_pretrained(\"/scratch/10992/liaorunlong93/random_flow_toys/checkpoints/flow_mnist_unet_unconditional\", use_ema=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.samplers import EulerSampler\n",
    "from rectified_flow.utils import plot_cifar_results\n",
    "\n",
    "rf = RectifiedFlow(\n",
    "    data_shape=(1, 28, 28),\n",
    "    velocity_field=flow_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "sampler = EulerSampler(rectified_flow=rf, num_samples=50, num_steps=200)\n",
    "x_1 = sampler.sample_loop().trajectories[-1]\n",
    "\n",
    "plot_cifar_results(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f0a9d",
   "metadata": {},
   "source": [
    "## Exact Jacobian calculation with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def divergence_exact_manual(v_func, x_t, t):\n",
    "    device, dtype = x_t.device, x_t.dtype\n",
    "    B = x_t.shape[0]\n",
    "    div = torch.empty(B, device=device, dtype=dtype)\n",
    "\n",
    "    for b in tqdm(range(B), desc=\"Computing exact divergence\"):\n",
    "        xb = x_t[b:b+1].detach().requires_grad_(True)\n",
    "        tb = t[b:b+1]\n",
    "\n",
    "        def f(inp):\n",
    "            return v_func(inp, tb)\n",
    "\n",
    "        J = jacobian(f, xb, vectorize=False, create_graph=False)\n",
    "        D = xb.numel()\n",
    "        div[b] = J.reshape(D, D).diagonal().sum().to(dtype)\n",
    "        del J\n",
    "        \n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557fac7",
   "metadata": {},
   "source": [
    "### Divergence Estimation via the Hutchinson Trick\n",
    "\n",
    "For a vector field $v(x,t)$ with Jacobian $J(x) = \\dfrac{\\partial v}{\\partial x}$, the divergence is\n",
    "$$\n",
    "\\operatorname{div} v(x,t) = \\operatorname{tr}\\, J(x).\n",
    "$$\n",
    "Computing $\\operatorname{tr} J$ via an explicit Jacobian is expensive. The Hutchinson identity gives an unbiased estimator:\n",
    "$$\n",
    "\\operatorname{tr} J = \\mathbb{E}_{\\varepsilon}\\!\\left[\\varepsilon^\\top J \\varepsilon\\right],\n",
    "$$\n",
    "where $\\varepsilon$ can be Rademacher ($\\pm1$ w.p. 1/2) or standard Gaussian. A Monte Carlo estimate is\n",
    "$$\n",
    "\\widehat{\\operatorname{div}}(x,t) = \\frac{1}{M}\\sum_{i=1}^{M} \\varepsilon_i^\\top (J\\,\\varepsilon_i),\n",
    "$$\n",
    "which only requires Jacobian–vector products (JVP) or vector–Jacobian products (VJP), avoiding materializing the full Jacobian.\n",
    "\n",
    "**Implementation notes**\n",
    "- `eps_dist=\"rademacher\"` often yields slightly lower variance; `\"normal\"` works too.\n",
    "- Use forward-mode `jvp` to get $J\\varepsilon$, or a single backward (VJP) to get $J^\\top \\varepsilon$ and then dot with $\\varepsilon$.\n",
    "- Compute per-sample inner products in the batch and average across $M$ probes; `M=1–4` is typically sufficient.\n",
    "- When using fp16/bf16, accumulate in fp32 for numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761184e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jvp as jvp_func\n",
    "\n",
    "def divergence_hutchinson(\n",
    "    v_func,\n",
    "    x_t: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    n_samples: int = 1,\n",
    "    eps_dist: str = \"rademacher\",\n",
    "    method: str = \"jvp\",\n",
    "    generator: torch.Generator | None = None,\n",
    ") -> torch.Tensor:\n",
    "    device, dtype = x_t.device, x_t.dtype\n",
    "    B = x_t.shape[0]\n",
    "    D = x_t.numel() // B\n",
    "\n",
    "    x_flat = x_t.detach().view(B, D)\n",
    "\n",
    "    acc_dtype = torch.float32 if dtype in (torch.float16, torch.bfloat16) else dtype\n",
    "    acc = torch.zeros(B, device=device, dtype=acc_dtype)\n",
    "\n",
    "    def _sample_eps_flat():\n",
    "        if eps_dist == \"rademacher\":\n",
    "            r = torch.randint(0, 2, (B, D), device=device, generator=generator)\n",
    "            eps = (r * 2 - 1).to(acc_dtype)\n",
    "        elif eps_dist == \"normal\":\n",
    "            eps = torch.randn((B, D), device=device, dtype=acc_dtype, generator=generator)\n",
    "        else:\n",
    "            raise ValueError(\"eps_dist must be 'rademacher' or 'normal'\")\n",
    "        return eps.to(dtype)\n",
    "\n",
    "    if method == \"jvp\":\n",
    "        def f_flat(inp_flat):\n",
    "            x4 = inp_flat.view(B, *x_t.shape[1:])\n",
    "            y4 = v_func(x4, t)\n",
    "            return y4.view(B, D)\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            eps_flat = _sample_eps_flat()\n",
    "            _, jvp_out = jvp_func(f_flat, (x_flat,), (eps_flat,), strict=False)  # <-- fix\n",
    "            acc += (eps_flat * jvp_out).sum(dim=1).to(acc_dtype)\n",
    "\n",
    "        return (acc / n_samples).to(dtype)\n",
    "\n",
    "    elif method == \"vjp\":\n",
    "        for _ in range(n_samples):\n",
    "            eps = _sample_eps_flat().view_as(x_t).to(dtype)\n",
    "            x_req = x_t.detach().clone().requires_grad_(True)\n",
    "            v = v_func(x_req, t)\n",
    "            s_per_sample = (v * eps).view(B, -1).sum(dim=1)\n",
    "            gx = torch.autograd.grad(s_per_sample.sum(), x_req, create_graph=False, retain_graph=False)[0]\n",
    "            acc += (gx * eps).view(B, -1).sum(dim=1).to(acc_dtype)\n",
    "\n",
    "        return (acc / n_samples).to(dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'jvp' or 'vjp'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ad356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = batch[0].to(device)[:5]\n",
    "t = torch.rand(x_1.shape[0], device=device)\n",
    "x_t = t[:, None, None, None] * x_1 + (1 - t)[:, None, None, None] * torch.randn_like(x_1)\n",
    "\n",
    "div_exact = divergence_exact_manual(flow_model, x_t, t)\n",
    "div_approx = divergence_hutchinson(flow_model, x_t, t, n_samples=10, method=\"vjp\")\n",
    "\n",
    "print(\"Divergence shape:\", div_exact.shape)\n",
    "print(\"div exact:\", div_exact)\n",
    "print(\"div approx:\", div_approx)\n",
    "print(\"Max absolute error:\", (div_exact - div_approx).abs().max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vista-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
