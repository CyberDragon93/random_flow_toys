{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d4d992",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Try flow MNIST with UNet MLP, UNet / DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f20ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rectified_flow.rectified_flow import RectifiedFlow\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48eb2cd",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## Load dataset, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fa14fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "transform_list = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "]\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms.Compose(transform_list)\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)  # torch.Size([256, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817da497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"mlp\"    # \"mlp\" or \"unet\" or \"dit\"\n",
    "from rectified_flow.models.enhanced_mlp import VarMLP\n",
    "from rectified_flow.models.utils import EMAModel\n",
    "\n",
    "if model_type == \"mlp\":\n",
    "    from rectified_flow.models.enhanced_mlp import MLPVelocity\n",
    "    flow_model = MLPVelocity(\n",
    "        dim=28*28,\n",
    "    )\n",
    "elif model_type == \"unet\":\n",
    "    from rectified_flow.models.unet import SongUNet, SongUNetConfig\n",
    "    config = SongUNetConfig(\n",
    "        img_resolution = 28,\n",
    "        in_channels = 1,                  # Number of color channels at input.\n",
    "        out_channels = 1,                 # Number of color channels at output.\n",
    "        label_dim = 0,                    # Number of class labels, 0 = unconditional.\n",
    "        augment_dim = 0,                   # Augmentation label dimensionality, 0 = no augmentation.\n",
    "\n",
    "        model_channels = 64,               # Base multiplier for the number of channels.\n",
    "        channel_mult = [2, 2],               # Channel multipliers for each resolution.\n",
    "        channel_mult_emb = 2,                # Multiplier for the dimensionality of the embedding vector.\n",
    "        num_blocks = 3,                      # Number of residual blocks per resolution.\n",
    "        attn_resolutions = [16],             # Resolutions at which to apply attention.\n",
    "        dropout = 0.13,                      # Dropout probability of intermediate activations.\n",
    "        label_dropout = 0.0,                 # Dropout probability of class labels for classifier-free guidance.\n",
    "        embedding_type = \"positional\",        # Timestep embedding type: 'positional' or 'fourier'.\n",
    "        channel_mult_time = 1,                 # Timestep embedding size: 1 for DDPM++, 2 for NCSN++.\n",
    "        encoder_type = \"standard\",              # Encoder architecture: 'standard' or 'residual'.\n",
    "        decoder_type = \"standard\",              # Decoder architecture: 'standard' or 'residual'.\n",
    "        resample_filter = [1, 1]\n",
    "    )\n",
    "    flow_model = SongUNet(config)\n",
    "elif model_type == \"dit\":\n",
    "    raise NotImplementedError\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "logvar = VarMLP().to(device)\n",
    "flow_model = flow_model.to(device)\n",
    "\n",
    "ema_flow = EMAModel(flow_model, ema_halflife_kimg=1.0, ema_rampup_ratio=0.05)\n",
    "ema_logvar = EMAModel(logvar, ema_halflife_kimg=1.0, ema_rampup_ratio=0.05)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(flow_model.parameters()) + list(logvar.parameters()),\n",
    "    lr=1e-4, weight_decay=0.01,\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "compiled_flow = torch.compile(flow_model, mode=\"reduce-overhead\", fullgraph=False, dynamic=False)\n",
    "compiled_logvar = torch.compile(logvar, mode=\"reduce-overhead\", fullgraph=False, dynamic=False)\n",
    "\n",
    "compiled_flow.train(); compiled_logvar.train()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4fcfb",
   "metadata": {},
   "source": [
    "## Train Unconditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train = RectifiedFlow(\n",
    "    data_shape=(1, 28, 28),\n",
    "    velocity_field=compiled_flow,\n",
    "    train_time_distribution=\"uniform\",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vista-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
